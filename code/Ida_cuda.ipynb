{
 "cells": [
  {
   "cell_type": "code",
   "id": "95f5507c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:05:10.463822Z",
     "start_time": "2025-10-27T11:05:10.459373Z"
    }
   },
   "source": [
    "# 1. 导入所有依赖库（新增numpy，用于固定随机种子）\n",
    "import pandas as pd\n",
    "import jieba.posseg as pseg\n",
    "from gensim import corpora\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Gamma\n",
    "import numpy as np  # 新增：用于固定numpy随机种子\n",
    "\n",
    "\n",
    "# 2. 基础配置与GPU验证（先确认GPU可用）+ 固定随机种子（核心新增）\n",
    "def set_random_seed(seed=2023):\n",
    "    \"\"\"固定所有随机种子，确保结果可重复\"\"\"\n",
    "    # 固定PyTorch随机种子\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # 多GPU时用\n",
    "    # 固定NumPy随机种子（代码中用numpy处理数据）\n",
    "    np.random.seed(seed)\n",
    "    # 禁用PyTorch的cuDNN随机性（确保卷积等操作一致）\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "466c5ed3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:09:40.095712Z",
     "start_time": "2025-10-27T11:09:40.089805Z"
    }
   },
   "source": [
    "\n",
    "# 调用函数固定随机种子（seed=42是常用值，可自行修改）\n",
    "set_random_seed(seed=2023)"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "d0f02a97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:11:51.359642Z",
     "start_time": "2025-10-27T11:11:43.815762Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"当前运行设备：{device}\")  # 确保输出\"cuda\"\n",
    "print(f\"GPU是否可用：{torch.cuda.is_available()}\")\n",
    "\n",
    "\n",
    "# 3. 读取并预处理数据（过滤空值+空文档）\n",
    "def load_and_preprocess_data(csv_path):\n",
    "    # 读取CSV\n",
    "    df = pd.read_csv(csv_path, encoding=\"gbk\")\n",
    "    # 过滤message为空的行\n",
    "    df = df.dropna(subset=[\"message\"]).reset_index(drop=True)\n",
    "    messages = df[\"message\"].tolist()\n",
    "    \n",
    "    # 停用词（内置，无需额外文件）\n",
    "    stopwords = {\n",
    "        \"我\", \"你\", \"他\", \"她\", \"它\", \"我们\", \"你们\", \"他们\", \"的\", \"了\", \"是\", \"在\", \n",
    "        \"有\", \"和\", \"及\", \"与\", \"也\", \"还\", \"都\", \"很\", \"挺\", \"太\", \"非常\", \"呵呵\", \n",
    "        \"哈哈\", \"嗯\", \"哦\", \"呀\", \"啊\", \"啦\", \"吧\", \"呢\", \"吗\", \"今天\", \"昨天\", \"明天\",\n",
    "        \"。\", \"，\", \"、\", \"；\", \"：\", \"？\", \"！\", \"（\", \"）\", \"【\", \"】\", \"《\", \"》\"\n",
    "    }\n",
    "    keep_flags = ['n', 'v', 'a', 'vn']  # 保留名词、动词、形容词\n",
    "    \n",
    "    # 分词+过滤（关键：过滤空文档）\n",
    "    processed_words = []\n",
    "    valid_indices = []  # 记录有效文档的索引（用于后续筛选df）\n",
    "    for idx, msg in enumerate(messages):\n",
    "        # 清洗特殊字符（保留中文、字母、数字）\n",
    "        msg_clean = \"\".join([c for c in str(msg) if c.isalnum() or '\\u4e00' <= c <= '\\u9fff'])\n",
    "        # 分词+词性过滤\n",
    "        words = [w for w, f in pseg.cut(msg_clean) if f in keep_flags and w not in stopwords and len(w) > 1]\n",
    "        if words:  # 只保留有有效词的文档\n",
    "            processed_words.append(words)\n",
    "            valid_indices.append(idx)\n",
    "    \n",
    "    # 筛选有效文档的df（避免后续索引不匹配）\n",
    "    df_valid = df.iloc[valid_indices].reset_index(drop=True)\n",
    "    print(f\"原始数据量：{len(messages)}条 → 过滤后有效数据量：{len(processed_words)}条\")\n",
    "    return df_valid, processed_words\n",
    "\n",
    "\n",
    "# 4. 构建词典与GPU张量（适配PyTorch）\n",
    "def build_corpus_and_tensor(processed_words, device):\n",
    "    # 构建gensim词典（过滤低频词）\n",
    "    dictionary = corpora.Dictionary(processed_words)\n",
    "    dictionary.filter_extremes(no_below=2)  # 保留至少在2条文档中出现的词\n",
    "    n_words = len(dictionary)\n",
    "    n_docs = len(processed_words)\n",
    "    \n",
    "    # 词袋向量转PyTorch GPU张量（避免稀疏矩阵问题）\n",
    "    doc_word_tensor = torch.zeros((n_docs, n_words), dtype=torch.float32, device=device)\n",
    "    for doc_idx, bow in enumerate([dictionary.doc2bow(words) for words in processed_words]):\n",
    "        for word_idx, count in bow:\n",
    "            doc_word_tensor[doc_idx, word_idx] = count\n",
    "    \n",
    "    return dictionary, doc_word_tensor, n_words, n_docs\n",
    "\n",
    "\n",
    "# 5. 稳定版GPU-LDA模型（Gamma初始化+防nan）\n",
    "class StableLDA(nn.Module):\n",
    "    # 修复：新增n_docs参数（避免之前的NameError）\n",
    "    def __init__(self, n_topics, n_words, n_docs, alpha=1.0, beta=0.1, device=device):\n",
    "        super().__init__()\n",
    "        self.n_topics = n_topics\n",
    "        self.n_words = n_words\n",
    "        self.n_docs = n_docs  # 存储文档数\n",
    "        \n",
    "        # 关键：Gamma分布初始化（因已固定随机种子，每次初始化参数完全一致）\n",
    "        self.log_topic_word = nn.Parameter(\n",
    "            torch.log(\n",
    "                Gamma(concentration=beta*10, rate=10).sample((n_topics, n_words)).to(device)\n",
    "            )\n",
    "        )\n",
    "        self.log_doc_topic = nn.Parameter(\n",
    "            torch.log(\n",
    "                Gamma(concentration=alpha*10, rate=10).sample((n_docs, n_topics)).to(device)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self):\n",
    "        # 计算概率分布（softmax确保和为1）\n",
    "        doc_topic = torch.softmax(self.log_doc_topic, dim=1)  # 文档→主题\n",
    "        topic_word = torch.softmax(self.log_topic_word, dim=1)  # 主题→词\n",
    "        doc_word_pred = torch.matmul(doc_topic, topic_word)  # 文档→词预测\n",
    "        return doc_word_pred, doc_topic, topic_word\n",
    "\n",
    "\n",
    "# 6. 训练与结果提取（防nan损失+主题差异化）\n",
    "def train_lda_and_extract_results(\n",
    "    doc_word_tensor, n_topics, n_words, n_docs, df_valid, dictionary,\n",
    "    lr=0.001, n_epochs=1000, device=device\n",
    "):\n",
    "    # 初始化模型、优化器、损失函数（因固定种子，每次初始化完全一致）\n",
    "    model = StableLDA(\n",
    "        n_topics=n_topics, \n",
    "        n_words=n_words, \n",
    "        n_docs=n_docs,  # 传入文档数\n",
    "        device=device\n",
    "    ).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.KLDivLoss(reduction=\"sum\")  # LDA专用损失\n",
    "    \n",
    "    # 训练循环（因固定种子，每次训练的损失变化和参数更新完全一致）\n",
    "    print(\"\\n开始训练LDA模型...\")\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 前向传播\n",
    "        doc_word_pred, doc_topic, topic_word = model()\n",
    "        # 计算稳定损失（加1e-20避免log(0)和除以0）\n",
    "        doc_word_norm = doc_word_tensor / (doc_word_tensor.sum(dim=1, keepdim=True) + 1e-20)\n",
    "        loss = loss_fn(torch.log(doc_word_pred + 1e-20), doc_word_norm)\n",
    "        \n",
    "        # 反向传播+更新\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 每200次迭代打印损失（验证无nan，且每次运行损失值完全相同）\n",
    "        if (epoch + 1) % 200 == 0:\n",
    "            print(f\"迭代 {epoch+1}/{n_epochs} | 损失：{loss.item():.2f}\")\n",
    "    \n",
    "    # 提取结果（因固定种子，主题关键词和文档分配完全一致）\n",
    "    with torch.no_grad():\n",
    "        _, doc_topic, topic_word = model()\n",
    "        # 文档→主题结果（转CPU处理）\n",
    "        doc_topic_cpu = doc_topic.cpu().numpy()\n",
    "        df_valid[\"topic_id\"] = [doc_topic_cpu[i].argmax() for i in range(n_docs)]  # 主主题ID\n",
    "        df_valid[\"topic_prob\"] = [doc_topic_cpu[i].max() for i in range(n_docs)]    # 主主题概率\n",
    "        \n",
    "        # 主题→关键词结果（取前10个词）\n",
    "        topic_word_cpu = topic_word.cpu().numpy()\n",
    "        topic_keywords = []\n",
    "        for topic_idx in range(n_topics):\n",
    "            top_word_ids = topic_word_cpu[topic_idx].argsort()[-10:][::-1]  # 权重最高的10个词ID\n",
    "            top_words = [dictionary[id] for id in top_word_ids]\n",
    "            topic_keywords.append(f\"主题{topic_idx}：{', '.join(top_words)}\")\n",
    "    \n",
    "    return df_valid, topic_keywords\n",
    "\n",
    "\n",
    "# 7. 主函数（一键运行全流程）\n",
    "if __name__ == \"__main__\":\n",
    "    # ---------------------- 配置参数（只需改这里） ----------------------\n",
    "    CSV_PATH = \"南京景区-天气-社媒情感融合表.csv\"  # 替换为你的原始CSV路径\n",
    "    N_TOPICS = 4  # 主题数\n",
    "    SAVE_PATH = \"全量数据_带主题_最终版.csv\"  # 结果保存路径\n",
    "    \n",
    "    # ---------------------- 执行全流程 ----------------------\n",
    "    # 1. 数据预处理（确定性操作，无随机）\n",
    "    df_valid, processed_words = load_and_preprocess_data(CSV_PATH)\n",
    "    # 2. 构建词典与GPU张量（确定性操作，无随机）\n",
    "    dictionary, doc_word_tensor, n_words, n_docs = build_corpus_and_tensor(processed_words, device)\n",
    "    # 3. 训练模型并提取结果（因固定种子，结果完全可重复）\n",
    "    df_result, topic_keywords = train_lda_and_extract_results(\n",
    "        doc_word_tensor=doc_word_tensor,\n",
    "        n_topics=N_TOPICS,\n",
    "        n_words=n_words,\n",
    "        n_docs=n_docs,\n",
    "        df_valid=df_valid,\n",
    "        dictionary=dictionary\n",
    "    )\n",
    "    # 4. 保存结果并打印主题\n",
    "    df_result.to_csv(SAVE_PATH, index=False, encoding=\"utf-8\")\n",
    "    print(f\"\\n主题关键词列表：\")\n",
    "    for kw in topic_keywords:\n",
    "        print(kw)\n",
    "    print(f\"\\n结果已保存到：{SAVE_PATH}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前运行设备：cuda\n",
      "GPU是否可用：True\n",
      "原始数据量：5519条 → 过滤后有效数据量：4365条\n",
      "\n",
      "开始训练LDA模型...\n",
      "迭代 200/1000 | 损失：26679.63\n",
      "迭代 400/1000 | 损失：25596.54\n",
      "迭代 600/1000 | 损失：24599.92\n",
      "迭代 800/1000 | 损失：23693.67\n",
      "迭代 1000/1000 | 损失：22890.96\n",
      "\n",
      "主题关键词列表：\n",
      "主题0：没有, 世界, 开心, 民国, 感受, 震撼, 出来, 生活, 灵谷, 演唱会\n",
      "主题1：落羽杉, 开始, 动物园, 游客, 美好, 音乐, 最美, 遇见, 游园, 适合\n",
      "主题2：故事, 漂亮, 酒家, 愉快, 举手, 关注, 日子, 照片, 美龄, 结束\n",
      "主题3：日落, 打卡, 总统府, 好看, 自由, 分享, 风景区, 图片, 苦涩, 大道\n",
      "\n",
      "结果已保存到：全量数据_带主题_最终版.csv\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T10:55:45.999061Z",
     "start_time": "2025-10-27T10:55:45.996593Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "17bb0177bd37976f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6da01798eec5cb1d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
